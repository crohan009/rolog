---
layout: post
comments: false
title: "CoFEx - Code Feature Extractor"
date: 2020-03-01 00:01:01
tags: 
---

> CoFEx is a probabilistic deep neural network model designed to extract features from code (any programming lauguage) by combining XLNet's laugage model (treating code as a natural language) and Gated-Graph Neural networks (treating code as a hierarchical graph), thereby bi-modally modelling code in order to create embeddings for programming languages for numerous applications such as code auto-completion, code prediction, bug detection and correction. 

<!--more-->

---
<h3> Contents </h3>

{: class="table-of-content"}
* TOC
{:toc}

---

## Intro

<p>
According to the <b>Naturalness Hypothesis</b> [1], software is a form of human communication, and just like natural languages, possesses probabilistic and statistical properties that can be exploitedby machine learning (ML).  This has motivated the bimodal modelling of code that allows code to beinterpreted by humans and computers. Therefore, theoretically as our models of natural language improve, so will our ability to engineer and automate all aspects of software analysis and code generation. </p>

<p>
The trajectory of <b>ML on big code</b> has therefore followed the numerous break-throughsin the domain of Natural Language Understanding.  Token level models were the first to use the N-gram language models [2] to model code token statistics.  This was followed by sequence-models that utilized deep recurrent neural network models (RNNs and LSTMs) to better encode long and short term dependencies between tokens. Finally, neural attention models [3,4] have been the latest successors which work by paying "attention" to thelocal context of code to obtain token embeddings. </p>

<p>
The research in the field of Natural Language has been standardised with the <b>GLUE benchmark</b> [5] for comparing the numerous state of the art models. Consequently, there has been a progressive improvement of probabilistic language modelling with its applications trickling into everyday technologies such a word predictive keyboards and gmail’s sentence level email predictions. </p>

## Context embeddings (A very Brief History)

- In language processing, **embeddings** are numbers or vectors of numbers that are assigned to tokens (such as words in the english dictionary).
- Therefore, a word, a sentence, a paragraph, or an entire document may be represented by a vector of numbers.
- **Context embeddings** encode the context within which a particular token lies.

In general, there are two methods employed for extracting context embeddings ...

### 1. Autoregressive (AR) methods

Given a text sequence $$ x = [x_1,..., x_T]$$. <br>
AR models maximize the following likelihood objective function: <br>

$$ 
\underset{\theta}{\text{max}} \ \ log \ p_{\theta}(x) = \sum_{t=1}^T log p_{\theta}(x_t | x_{<T})
= \sum_{t=1}^T log \frac{exp \ ( h_{\theta}{(x_{1:t-1})}^T e(x_t))}{ \sum_{x'} exp \ ( h_{\theta}{(x_{1:t-1})}^T e(x')) }
$$ 

where, 
 - $$h_{\theta}{(x_{1:t-1})}^T$$ is the **context representation**, 
 - $$e(x_t)$$ is the **embedding of x**, and 
 - $$exp \ ( h_{\theta}{(x_{1:t-1})}^T e(x_t))$$ is the **dot-product similarity** between a word embedding and its context representaiton. 

The context rep may be producted by any learning model (eg: language modelling, RNN, LSTM, Attention).

<h4> Limitations </h4>

1. Dependencies between the words on the left and the words on the right is missing
2. Even when combining forward and backward LMs (with bi-directional RNNs)

<h4><i> Examples </i></h4>

#### 1.1. Word2Vec

<br>

![word2vec of C/C++ tokens ]({{ '/assets/images/C_word2vec_embedding.png' | relative_url }})
{: style="width: 90%;" class="center"}
*Fig. 1. A word2vec embedding of tokens from C/C++ source code. <br> Source: [Harer et al - Automated software vulnerability detection with machine learning](https://arxiv.org/pdf/1803.04497.pdf)*

---

#### 1.2. RNN and its Variants (Sequence to Sequence models)

![seq2seq models]({{ '/assets/images/seq_2_seq.png' | relative_url }})
{: style="width: 90%;" class="center"}
*Fig. 2. Seq2Seq:  encoder-decoder models that maps input sequences to output of sequences.*

---

#### 1.3. ELMo [(Embeddings from Language Models)](https://arxiv.org/abs/1802.05365)

![ELMo ]({{ '/assets/images/ELMo.png' | relative_url }})
{: style="width: 150%;" class="center"}
*Fig. 3. ELMo's Context embedding extraction mechanism.*

---


### 2. Autoencoding (AE) methods

Given a text sequence: $$ x = [x_1,..., x_T]$$ <br>
A corrupted version of x by randomly masking 15% (BERT) of tokens: $$\hat{x}$$ <br>
Let the masked tokens be: $$\bar{x}$$ <br>
AE methods maximize the following objective:

$$
\underset{\theta}{\text{max}} \ \ log \ p_{\theta}( \bar{x} | \hat{x}) \approx \sum_{t=1}^T m_t log \ p_{\theta} (x_t | \hat{x}) = \sum_{t=1}^T m_t log \frac{ exp( H_{\theta} (\hat{x}))_{t}^T e(x_t) }{ \sum_{x'} exp( H_{\theta} (\hat{x}))_{t}^T ) e(x')}
$$

where, 
 - $$m_t$$: Indicates if $$x_t$$ is masked or not (0 or 1)
 - $$H_{\theta}(.)$$ : **Transformer**

<h4><i> Examples </i></h4>

#### 2.1. Attention Transformers

![Attention Transformers]({{ '/assets/images/att_transf.png' | relative_url }})
{: style="width: 150%;" class="center"}
*Fig. 4. Attention transformers. Source: [Vaswani et al - Attention is all you need.](https://arxiv.org/abs/1706.03762)*

#### 2.2. BERT

BERT brings together concepts form multiple papers:

![BERT's ideas]({{ '/assets/images/BERT_ideas.png' | relative_url }})
{: style="width: 60%;" class="center"}
*Fig. 5. BERT's ideas.*

- ELM0 : bi-directional context embeddings
- ULMFit : Pretraining / Finetuning for NLP
- Transformers: Enocoder - Decoder transformers
- OpenAI GPT: Decoder stacked transformers 

<h4> BERT's Pre-training </h4>

![BERT pretraining]({{ '/assets/images/BERT_pretraining.png' | relative_url }})
{: style="width: 150%;" class="center"}
*Fig. 6. BERT pre-trianing tasks: (1) Masked Language Modelling, and (2) Two Sentence Task.*

<h4> BERT's Fine-tuning </h4>

![BERT finetuning]({{ '/assets/images/BERT_finetuning.png' | relative_url }})
{: style="width: 150%;" class="center"}
*Fig. 6. BERT fine-trianing tasks (GLUE Benchmarks).*

--- 

<h4> Limitations of BERT </h4>

1. When predicting a masked token, other masked tokens are in the input - What if masked tokens are dependent?
2. Consecutive segment encodings are detached.
3. BERT uses Absolute positional encodings - consecutive segments do not have any relative positional information

#### 2.3. Graph Attention Methods: [Gated Graph Attention Networks](https://arxiv.org/abs/1803.07294)

![GaAN]({{ '/assets/images/GaAN.png' | relative_url }})
{: style="width: 150%;" class="center"}
*Fig. 7. Gated Graph Attention Networks.*



## CoFEx

To best model bimodal code properties, we look at code as (1) a sequence of natural language tokens which have long and short term bi-directional dependencies and (2) a programming language wherein there exist a structural hierarchy amongst the internal-types of the language. The latter can be derived from the **Abstract Syntax Tree AST)**  of any piece of code. Therefore, CoFEx extracts code features by two mechanisms: **language modelling** of code and **graph modelling** of the code AST tree.

The language model's architecture is based on the permutation language model utilized by [XLNet](https://arxiv.org/abs/1906.08237). This model performs an auto-regressive training on all possible permutations of code sequences by using stack of encoder blocks that perform the two-stream relative self-attention. We introduce graph modelling into the same framework by augmenting each encoder block to have two sub-blocks, the Language Model and the Graph Model. The Graph Model utilizes the Gated Graph Attention network ([GaAN](https://arxiv.org/abs/1803.07294)) to allow our model to selectively pay attention to the graph neighbourhood of tokens within the code AST tree. This equips our model to simultaneously embed each code token with its local context as well as its role in the code AST tree hierarchy.

<i>**(to be continued)**</i>

## References

[1] Miltiadis Allamanis, Earl T Barr,  Premkumar Devanbu,  and Charles Sutton. A survey ofmachine learning for big code and naturalness.ACM Computing Surveys (CSUR), 51(4):81,2018.

[2] Veselin Raychev, Martin Vechev, and Eran Yahav. Code completion with statistical languagemodels. InAcm Sigplan Notices, volume 49, pages 419–428. ACM, 2014.

[3] Uri Alon, Meital Zilberstein, Omer Levy, and Eran Yahav.  code2vec:  Learning distributedrepresentations of code.Proceedings of the ACM on Programming Languages, 3(POPL):40,2019.

[4] Srinivasan Iyer, Ioannis Konstas, Alvin Cheung, and Luke Zettlemoyer. Summarizing sourcecode  using  a  neural  attention  model.   InProceedings of the 54th Annual Meeting of theAssociation for Computational Linguistics (Volume 1: Long Papers), pages 2073–2083, 2016

[5] Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R Bowman.Glue: A multi-task benchmark and analysis platform for natural language understanding.arXivpreprint arXiv:1804.07461, 2018

*Please don't hesitate to contact me with any errors. I'll be sure to correct them right away !*